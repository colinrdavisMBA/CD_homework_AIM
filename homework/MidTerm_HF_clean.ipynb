{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-15 14:40:35 - Loaded .env file\n"
     ]
    }
   ],
   "source": [
    "#import necessary packages\n",
    "import os\n",
    "from openai import AsyncOpenAI  # importing openai for API usage\n",
    "import chainlit as cl  # importing chainlit for our app\n",
    "from chainlit.playground.providers import ChatOpenAI  # importing ChatOpenAI tools\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.retrievers import MultiQueryRetriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain import hub\n",
    "\n",
    "\n",
    "\n",
    "#from langchain.utils import itemgetter, RunnablePassthrough\n",
    "#from langchain.chains import build_chain\n",
    "#from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#from langchain_community.document_loaders import PyMuPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load environment var\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-15 14:40:36 - Loading faiss.\n",
      "2024-03-15 14:40:36 - Successfully loaded faiss.\n"
     ]
    }
   ],
   "source": [
    "#load in embeddings model\n",
    "out_fp = './data'\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "#vector_store = FAISS.from_documents(documents, embeddings)\n",
    "faiss_fn = 'nvidia_10k_faiss_index.bin'\n",
    "vector_store=FAISS.load_local(out_fp+faiss_fn, embeddings, allow_dangerous_deserialization=True)\n",
    "retriever = vector_store.as_retriever()\n",
    "openai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOpenAI Templates\n",
    "template = \"\"\"Answer the question based only on the following context. If you cannot answer the question with the context, respond with 'I don't know'. You'll get a big bonus and a potential promotion if you provide a high quality answer:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create chain\n",
    "retrieval_qa_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "primary_qa_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "advanced_retriever = MultiQueryRetriever.from_llm(retriever=retriever, llm=primary_qa_llm)\n",
    "document_chain = create_stuff_documents_chain(primary_qa_llm, retrieval_qa_prompt)\n",
    "retrieval_chain = create_retrieval_chain(advanced_retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cl.on_chat_start  # marks a function that will be executed at the start of a user session\n",
    "async def start_chat():\n",
    "    settings = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"temperature\": 0,\n",
    "        \"max_tokens\": 250,\n",
    "        \"top_p\": 1,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0,\n",
    "    }\n",
    "\n",
    "    cl.user_session.set(\"settings\", settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cl.on_message  # marks a function that should be run each time the chatbot receives a message from a user\n",
    "async def main(message: cl.Message):\n",
    "    settings = cl.user_session.get(\"settings\")\n",
    "\n",
    "    # Use the retrieval_augmented_qa_chain_openai pipeline with the user's question\n",
    "    question = message.content  # Extracting the question from the message content\n",
    "    response = await retrieval_chain.invoke({\"question\": question})  # Invoke the pipeline\n",
    "\n",
    "    # Extract the response content and context documents\n",
    "    response_content = response[\"response\"].content\n",
    "    context_documents = '\\n'.join([document.page_content for document in response[\"context\"]])\n",
    "    page_numbers = set([document.metadata['page'] for document in response[\"context\"]])\n",
    "\n",
    "    # Stream the response content back to the user\n",
    "    msg = cl.Message(content=\"\")\n",
    "    await msg.stream_token(response_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook MidTerm_HF_clean.ipynb to script\n",
      "[NbConvertApp] Writing 3630 bytes to MidTerm_HF_clean.py\n"
     ]
    }
   ],
   "source": [
    "#!jupyter nbconvert --to script MidTerm_HF_clean.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cd_llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
