{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZO_xiWUoSKX"
      },
      "source": [
        "# Embeddings & You - A Brief Introduction to Embeddings in Machine Learning\n",
        "\n",
        "If you've toyed with LangChain, LlamaIndex, or even OpenAI's `ada` model - you've likely run into the word: \"Embeddings\" a few time.\n",
        "\n",
        "They've had a recent surge in popularity due to the profliferation of Retrieval Augmented Generation, but they've been around for a very long time.\n",
        "\n",
        "If you come from an NLP background, embeddings are something you might be intimately familiar with - otherwise, you might find the topic a bit...dense. (this attempt at a joke will make more sense later)\n",
        "\n",
        "In all seriousness, embeddings are a powerful piece of the NLP puzzle, so let's dive in!\n",
        "\n",
        "> NOTE: While this notebook language/NLP-centric, embeddings have uses beyond just text!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P70JnMeToYd3"
      },
      "source": [
        "### Why Do We Even Need Embeddings?\n",
        "\n",
        "In order to fully understand what Embeddings are, we first need to understand why we have them:\n",
        "\n",
        "Machine Learning algorithms, ranging from the very big to the very small, all have one thing in common:\n",
        "\n",
        "*They need numeric inputs.*\n",
        "\n",
        "So we need a process by which to translate the domain we live in, dominated by images, audio, language, and more, into the domain of the machine: Numbers.\n",
        "\n",
        "Another thing we want to be able to do is capture \"semantic information\" about words/phrases so that we can use algorithmic approaches to determine if words are closely related or not!\n",
        "\n",
        "So, we need to come up with a process that does these two things well:\n",
        "\n",
        "1. Convert non-numeric data into numeric-data\n",
        "2. Capture potential semantic relationships between individual pieces of data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AkTA6O8boBu"
      },
      "source": [
        "## Training Word2Vec from Scratch\n",
        "\n",
        "Now that we have a bit of background on Embeddings - let's look at what it takes to create our own embeddings using Word2Vec!\n",
        "\n",
        "We'll be leveraging the `gensim` library, which you can read all about [here](https://pypi.org/project/gensim/).\n",
        "\n",
        "Before we begin training, however, we need some data!\n",
        "\n",
        "Let's use the Wikipedia pages for Barbie and Oppenheimer as examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE0PCOWjdHO8"
      },
      "source": [
        "### Data Collection\n",
        "\n",
        "We'll leverage the `wikipedia` library, and `langchain`s `WikipediaLoader` to obtain our Wikipedia data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f1bHSdU9a8Tu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -U -q wikipedia langchain langchain_community langchain_openai lxml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME25TAtVdPmE"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "\n",
        "barbie_docs = WikipediaLoader(\n",
        "    query=\"Barbie\",\n",
        "    load_max_docs=5,\n",
        "    doc_content_chars_max=1_000_000\n",
        "    ).load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLV9ZD4ReBrw"
      },
      "outputs": [],
      "source": [
        "len(barbie_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-_YAa6MeWmP"
      },
      "outputs": [],
      "source": [
        "oppenheimer_docs = WikipediaLoader(\n",
        "    query=\"Oppenheimer\",\n",
        "    load_max_docs=5,\n",
        "    doc_content_chars_max=1_000_000\n",
        "    ).load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_Tk5-_qehyb"
      },
      "outputs": [],
      "source": [
        "len(oppenheimer_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeJC376oep3N"
      },
      "source": [
        "Now that we have some text, we need to do some preprocessing! That's right - classic NLP!\n",
        "\n",
        "Let's begin by cleaning up our text, we'll:\n",
        "\n",
        "- Remove special characters\n",
        "- Remove stop words\n",
        "- Remove links\n",
        "- Convert to lowercase\n",
        "- Strip whitespace\n",
        "\n",
        "To do this, we'll need two main modules:\n",
        "\n",
        "- The `re` standard library module\n",
        "- `spacy`, another NLP library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uc-baY4hfN7c"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-XweJuKfl2w"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqLntUU6ftyl"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english')[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imiFMaxDssWf"
      },
      "source": [
        "####üèóÔ∏è Activity #1:\n",
        "\n",
        "What should the output format of the `preprocess_text` function be?\n",
        "\n",
        "Once you've determined the output format - please complete the code cell and ensure the appropriate format is returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSJqh7Cae2WY"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import List\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def preprocess_text(text: str) -> List[str]:\n",
        "  # remove links\n",
        "  text = re.sub(r\"YOUR PATTERN HERE\", \"\", text)\n",
        "  # remove all special characters (keep alphabet characters)\n",
        "  text = re.sub(\"YOUR PATTERN HERE\", \" \", text)\n",
        "  # tokenize text, make lowercase, and remove stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = word_tokenize(text)\n",
        "  filtered_tokens = [\n",
        "      ### YOUR CODE HERE\n",
        "  ]\n",
        "  return filtered_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TLBwJtff89s"
      },
      "source": [
        "Let's see how this works on some of our Wikipedia data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0akPT-BJgB6_"
      },
      "outputs": [],
      "source": [
        "preprocess_text(barbie_docs[0].page_content[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFUQpAJngOg0"
      },
      "source": [
        "###üèóÔ∏è Activity #2:\n",
        "\n",
        "What should the output format of the `sentence_tokenization` function be?\n",
        "\n",
        "Once you've determined the output format - please complete the code cell and ensure the appropriate format is returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goqsOzIlgF_x"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def sentence_tokenization(text: str) -> List[List[str]]:\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = ### YOUR CODE HERE\n",
        "    # Tokenize each sentence into words and store them in a list of lists\n",
        "    sentence_tokens = ### YOUR CODE HERE\n",
        "    return sentence_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anrS-SOth1tK"
      },
      "outputs": [],
      "source": [
        "sentence_tokenization(barbie_docs[0].page_content[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKLhQDeDiBv1"
      },
      "source": [
        "Perfect, with that, we're ready to create our corpus!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0Expe68h70P"
      },
      "outputs": [],
      "source": [
        "corpus = []\n",
        "\n",
        "for doc in barbie_docs:\n",
        "  corpus += sentence_tokenization(doc.page_content)\n",
        "\n",
        "for doc in oppenheimer_docs:\n",
        "  corpus += sentence_tokenization(doc.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2axftsiipxJ"
      },
      "source": [
        "### Training Word2Vec\n",
        "\n",
        "Now that we have our corpus set up, we can train our Word2Vec model.\n",
        "\n",
        "Training is straightforward, thanks to `gensim`, and more can be understood about the process by reading the paper - but let's see it in code!\n",
        "\n",
        "It's also worth considering/playing around with the `gensim` parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVb5Rye_ikjC"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6salAYUtcSr"
      },
      "source": [
        "###üèóÔ∏è Activity #3:\n",
        "\n",
        "Set appropriate hyperparameters for the gensim `Word2Vec` model.\n",
        "\n",
        "Please also describe what each parameter does, in your own words.\n",
        "\n",
        "> NOTE: Documentation is available [here](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSCza8r7i-Oh"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "VECTOR_SIZE = ### YOUR CODE HERE\n",
        "WINDOW = ### YOUR CODE HERE\n",
        "MIN_COUNT = ### YOUR CODE HERE\n",
        "SG = ### YOUR CODE HERE\n",
        "\n",
        "model = Word2Vec(\n",
        "    sentences=corpus,\n",
        "    vector_size=VECTOR_SIZE,\n",
        "    window=WINDOW,\n",
        "    min_count=MIN_COUNT,\n",
        "    sg=SG\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_LYRSDjjyA8"
      },
      "source": [
        "Blink and you'll miss it. You just trained an embeddings model!\n",
        "\n",
        "Let's try it out and see what we did!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaO35GCJjIlk"
      },
      "outputs": [],
      "source": [
        "model.wv[\"barbie\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9yYINaHj_qI"
      },
      "source": [
        "Finally! We see it: An embedding in the wild.\n",
        "\n",
        "Notice how we input a word, in this case Barbie, and we got back a 100-dimensional vector of floats.\n",
        "\n",
        "Let's see if we can't get back a list of similar vectors to the vector for \"barbie\", and \"oppenheimer\"!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3feBDOaj6WC"
      },
      "outputs": [],
      "source": [
        "model.wv.most_similar(positive=[\"barbie\"], topn=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrCY4Y4Rkn0N"
      },
      "outputs": [],
      "source": [
        "model.wv.most_similar(positive=[\"oppenheimer\"], topn=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFJQBuooli2S"
      },
      "source": [
        "Now, for the moment of truth, let's see if it can \"do the thing\" that is shown in every embeddings visualization ever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fFyEP8-mRw1"
      },
      "outputs": [],
      "source": [
        "ken_vec = model.wv[\"ken\"]\n",
        "man_vec = model.wv[\"man\"]\n",
        "mystery_vector = ken_vec - man_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfup5uT0nDci"
      },
      "outputs": [],
      "source": [
        "model.wv.most_similar(positive=[mystery_vector], topn=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGdGFwRjoJjg"
      },
      "source": [
        "And there we have it - embeddings, and a demonstration of what makes them so powerful!\n",
        "\n",
        "> Note: This is a very small sample size, and while this result is what we'd hope for - it is largely coincidental - this behaviour is expressed better in much larger corpus' of text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay6CQ4yRqjc9"
      },
      "source": [
        "## Fine-tuning an Embedding Model with Llama Index\n",
        "\n",
        "Now that we've seen where embeddings \"started\", as it were, let's see where they've gotten.\n",
        "\n",
        "In this section, we'll be fine-tuning Hugging Face's [sentence transformers](https://www.sbert.net/).\n",
        "\n",
        "Sentence Transformers leverages the work done in the [Sentence-BERT](https://arxiv.org/abs/1908.10084) paper. So while the idea of converting input text into a dense vector representation is the same, the way we got to those embeddings is a bit different.\n",
        "\n",
        "The code is largely adapted from [this](https://medium.com/llamaindex-blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971) amazing blog post by Jerry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtgGnYptrWhp"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q llama-index pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbd8TPSvq2WH"
      },
      "source": [
        "### Generating Synthetic Data\n",
        "\n",
        "Of course, when considering the easiest path forward to making our embeddings model better - it's tough to resist the siren's call of OpenAI's very cheap and very powerful models.\n",
        "\n",
        "Usually you'd need a team of people to generate high quality labelled data, so we'll shortcut that process by generating our own synthetic data!\n",
        "\n",
        "As always, we will first need to get some base data that we want to build our RAG pipeline on top of."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rKzD5ssq0BW"
      },
      "outputs": [],
      "source": [
        "!wget https://justcheckingonall.files.wordpress.com/2008/01/hhgtg1.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zyd5hCtNuGMF"
      },
      "outputs": [],
      "source": [
        "!wget https://justcheckingonall.files.wordpress.com/2008/01/hhgtg2.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnkjwoZtuSaF"
      },
      "outputs": [],
      "source": [
        "!wget https://justcheckingonall.files.wordpress.com/2008/01/hhgtg3.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9KsywtNuZa4"
      },
      "outputs": [],
      "source": [
        "TRAINING_FILES = [\"hhgtg1.pdf\", \"hhgtg2.pdf\"]\n",
        "EVAL_FILES = [\"hhgtg3.pdf\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2XgbleCuqHS"
      },
      "outputs": [],
      "source": [
        "%mkdir data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEyFZrGZGMrs"
      },
      "source": [
        "We'll set some paths to help the flow, if you're doing this locally and not in Colab this should let you do this process across sessions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bWFsR5hurtm"
      },
      "outputs": [],
      "source": [
        "TRAIN_CORPUS_FPATH = \"./data/train_corpus.json\"\n",
        "EVAL_CORPUS_FPATH = \"./data/eval_corpus.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7IImCdUGgX5"
      },
      "source": [
        "Next, we'll set up a helper function to help us convert our PDFs into a corpus - which is a collection of nodes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iM6P_y8Au5NF"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import SimpleNodeParser\n",
        "from llama_index.core.schema import MetadataMode\n",
        "\n",
        "def load_corpus(files, verbose=False):\n",
        "    if verbose:\n",
        "        print(f\"Loading files {files}\")\n",
        "\n",
        "    reader = SimpleDirectoryReader(input_files=files)\n",
        "    docs = reader.load_data()\n",
        "    if verbose:\n",
        "        print(f'Loaded {len(docs)} docs')\n",
        "\n",
        "    parser = SimpleNodeParser.from_defaults()\n",
        "    nodes = parser.get_nodes_from_documents(docs, show_progress=verbose)\n",
        "\n",
        "    if verbose:\n",
        "        print(f'Parsed {len(nodes)} nodes')\n",
        "\n",
        "    corpus = {node.node_id: node.get_content(metadata_mode=MetadataMode.NONE) for node in nodes}\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzRdkcxXvBaf"
      },
      "outputs": [],
      "source": [
        "train_corpus = load_corpus(TRAINING_FILES, verbose=True)\n",
        "eval_corpus = load_corpus(EVAL_FILES, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZS4dnwzGqWq"
      },
      "source": [
        "Let's write these data files out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SnQ9lECvXk4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(TRAIN_CORPUS_FPATH, 'w+') as f:\n",
        "    json.dump(train_corpus, f)\n",
        "\n",
        "with open(EVAL_CORPUS_FPATH, 'w+') as f:\n",
        "    json.dump(eval_corpus, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWk7VQX9Gsgp"
      },
      "source": [
        "### Preparing Fine-tuning Data\n",
        "\n",
        "Next up, we'll leverage `gpt-3.5-turbo` to create question and answer pairs that we will use to fine-tune our embeddings model.\n",
        "\n",
        "You could choose `gpt-4`, `claude` or substitute real human curated data for this step - but we'll see the processs through with `gpt-3.5-turbo` model as a demonstration!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uptw8N3e4Nl"
      },
      "outputs": [],
      "source": [
        "!pip install -qU llama-index-llms-openai llama-index-embeddings-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XskC4DB_vcxY"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import uuid\n",
        "\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsYjduTsvedJ"
      },
      "outputs": [],
      "source": [
        "TRAIN_QUERIES_FPATH = './data/train_queries.json'\n",
        "TRAIN_RELEVANT_DOCS_FPATH = './data/train_relevant_docs.json'\n",
        "\n",
        "EVAL_QUERIES_FPATH = './data/eval_queries.json'\n",
        "EVAL_RELEVANT_DOCS_FPATH = './data/eval_relevant_docs.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mimSXgLPvj9h"
      },
      "outputs": [],
      "source": [
        "with open(TRAIN_CORPUS_FPATH, 'r+') as f:\n",
        "    train_corpus = json.load(f)\n",
        "\n",
        "with open(EVAL_CORPUS_FPATH, 'r+') as f:\n",
        "    eval_corpus = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FVJPRyTHHsu"
      },
      "source": [
        "As always, we need our OpenAI API key!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UiZZh4FvpJZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKBqITuYHQ4D"
      },
      "source": [
        "Let's use this helper function to create our question answer pairs.\n",
        "\n",
        "We're going to use this prompt:\n",
        "\n",
        "```\n",
        "Context information is below.\n",
        "    \n",
        "---------------------\n",
        "{context_str}\n",
        "---------------------\n",
        "\n",
        "Given the context information and not prior knowledge.\n",
        "generate only questions based on the below query.\n",
        "\n",
        "You are a Teacher/ Professor. Your task is to setup \\\n",
        "{num_questions_per_chunk} questions for an upcoming \\\n",
        "quiz/examination. The questions should be diverse in nature \\\n",
        "across the document. Restrict the questions to the \\\n",
        "context information provided.\"\n",
        "```\n",
        "\n",
        "As you might be able to tell - we have the ability to control how many questions we generate, as well as the persona used to create the questions.\n",
        "\n",
        "The rest of the helper function is simply parsing the questions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSA2Bl3TvnRE"
      },
      "outputs": [],
      "source": [
        "def generate_queries(\n",
        "    corpus,\n",
        "    num_questions_per_chunk=2,\n",
        "    prompt_template=None,\n",
        "    verbose=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Automatically generate hypothetical questions that could be answered with\n",
        "    doc in the corpus.\n",
        "    \"\"\"\n",
        "    llm = OpenAI(model='gpt-3.5-turbo')\n",
        "\n",
        "    prompt_template = prompt_template or \"\"\"\\\n",
        "    Context information is below.\n",
        "\n",
        "    ---------------------\n",
        "    {context_str}\n",
        "    ---------------------\n",
        "\n",
        "    Given the context information and not prior knowledge.\n",
        "    generate only questions based on the below query.\n",
        "\n",
        "    You are a Teacher/ Professor. Your task is to setup \\\n",
        "    {num_questions_per_chunk} questions for an upcoming \\\n",
        "    quiz/examination. The questions should be diverse in nature \\\n",
        "    across the document. Restrict the questions to the \\\n",
        "    context information provided.\"\n",
        "    \"\"\"\n",
        "\n",
        "    queries = {}\n",
        "    relevant_docs = {}\n",
        "    for node_id, text in tqdm(corpus.items()):\n",
        "        query = prompt_template.format(context_str=text, num_questions_per_chunk=num_questions_per_chunk)\n",
        "        response = llm.complete(query)\n",
        "\n",
        "        result = str(response).strip().split(\"\\n\")\n",
        "        questions = [\n",
        "            re.sub(r\"^\\d+[\\).\\s]\", \"\", question).strip() for question in result\n",
        "        ]\n",
        "        questions = [question for question in questions if len(question) > 0]\n",
        "\n",
        "        for question in questions:\n",
        "            question_id = str(uuid.uuid4())\n",
        "            queries[question_id] = question\n",
        "            relevant_docs[question_id] = [node_id]\n",
        "    return queries, relevant_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huCqIo9ABN6e"
      },
      "source": [
        "> ### NOTE: The following cells take ~15min. to run - please ensure you have time for this step before continuing. I will provide the relevant data files if you wish to continue from this point! All of the data files can be found here: https://github.com/AI-Maker-Space/DataRepository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNkYQfCkv02J"
      },
      "outputs": [],
      "source": [
        "train_queries, train_relevant_docs = generate_queries(train_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWMi40eGyBlo"
      },
      "outputs": [],
      "source": [
        "eval_queries, eval_relevant_docs = generate_queries(eval_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkEbj6ByIO56"
      },
      "source": [
        "Let's save our data in an appropriate format for later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CO7ZURr0KRX"
      },
      "outputs": [],
      "source": [
        "with open(TRAIN_QUERIES_FPATH, 'w+') as f:\n",
        "    json.dump(train_queries, f)\n",
        "\n",
        "with open(TRAIN_RELEVANT_DOCS_FPATH, 'w+') as f:\n",
        "    json.dump(train_relevant_docs, f)\n",
        "\n",
        "with open(EVAL_QUERIES_FPATH, 'w+') as f:\n",
        "    json.dump(eval_queries, f)\n",
        "\n",
        "with open(EVAL_RELEVANT_DOCS_FPATH, 'w+') as f:\n",
        "    json.dump(eval_relevant_docs, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRxLy1uB1Ob5"
      },
      "outputs": [],
      "source": [
        "TRAIN_DATASET_FPATH = './data/train_dataset.json'\n",
        "EVAL_DATASET_FPATH = './data/eval_dataset.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_SWEK1O1P9W"
      },
      "outputs": [],
      "source": [
        "train_dataset = {\n",
        "    'queries': train_queries,\n",
        "    'corpus': train_corpus,\n",
        "    'relevant_docs': train_relevant_docs,\n",
        "}\n",
        "\n",
        "eval_dataset = {\n",
        "    'queries': eval_queries,\n",
        "    'corpus': eval_corpus,\n",
        "    'relevant_docs': eval_relevant_docs,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMLimGrv1eAL"
      },
      "outputs": [],
      "source": [
        "with open(TRAIN_DATASET_FPATH, 'w+') as f:\n",
        "    json.dump(train_dataset, f)\n",
        "\n",
        "with open(EVAL_DATASET_FPATH, 'w+') as f:\n",
        "    json.dump(eval_dataset, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUHK8gBfIv9U"
      },
      "source": [
        "### Fine-tuning Our Embeddings Model\n",
        "\n",
        "Finally, the set up is complete - and we can move on to fine-tuning our sentence transformer embedding model!\n",
        "\n",
        "The process is simplified considerably by how amazing the Hugging Face `sentence-transformer` library is, so let's jump straight in!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6QtdQve12PP"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98C2YcEj10di"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY3CCeScJboJ"
      },
      "source": [
        "We're going to use the `BAAI/bge-small-en` embedding model as an example, but you could use any of the `sentence-transformer` embeddings models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tiyPUjj11in"
      },
      "outputs": [],
      "source": [
        "model_id = \"BAAI/bge-small-en\"\n",
        "model = SentenceTransformer(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4wJgYpg1-Nh"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBP9U6w-Jyyo"
      },
      "source": [
        "Let's load our data into the desired format!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUy0HwPp1-TW"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers import InputExample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7OEQVmu2Kfb"
      },
      "outputs": [],
      "source": [
        "TRAIN_DATASET_FPATH = './data/train_dataset.json'\n",
        "VAL_DATASET_FPATH = './data/eval_dataset.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mw_hAOx_2NLz"
      },
      "outputs": [],
      "source": [
        "with open(TRAIN_DATASET_FPATH, 'r+') as f:\n",
        "    train_dataset = json.load(f)\n",
        "\n",
        "with open(VAL_DATASET_FPATH, 'r+') as f:\n",
        "    val_dataset = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tOJ3GIN2Odx"
      },
      "outputs": [],
      "source": [
        "dataset = train_dataset\n",
        "\n",
        "corpus = dataset['corpus']\n",
        "queries = dataset['queries']\n",
        "relevant_docs = dataset['relevant_docs']\n",
        "\n",
        "examples = []\n",
        "for query_id, query in queries.items():\n",
        "    node_id = relevant_docs[query_id][0]\n",
        "    text = corpus[node_id]\n",
        "    example = InputExample(texts=[query, text])\n",
        "    examples.append(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uAhnbW_LxaK"
      },
      "source": [
        "We're going to be leveraging `sentence_transformers` `MultipleNegativesRankingLoss` as our loss function.\n",
        "\n",
        "You can read more about it in the docs, [here](https://www.sbert.net/docs/package_reference/losses.html#multiplenegativesrankingloss).\n",
        "\n",
        "Note that there is [research](https://arxiv.org/pdf/1705.00652.pdf) that indicates that performance generally scales with `BATCH_SIZE`, but we're going to stick with an arbitrary 10 for the example in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwX2ltEj2RBn"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6LQuafa2R54"
      },
      "outputs": [],
      "source": [
        "loss = losses.MultipleNegativesRankingLoss(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCZwGmQY2Pv5"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 10\n",
        "\n",
        "loader = DataLoader(\n",
        "    examples, batch_size=BATCH_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpPrZ_6ZMOqh"
      },
      "source": [
        "We'll set up the `InformationRetrievalEvaluator` to determine performance during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCuVK5S22TnQ"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.evaluation import InformationRetrievalEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d90XGt92U1P"
      },
      "outputs": [],
      "source": [
        "dataset = val_dataset\n",
        "\n",
        "corpus = dataset['corpus']\n",
        "queries = dataset['queries']\n",
        "relevant_docs = dataset['relevant_docs']\n",
        "\n",
        "evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2GR2A28MUBD"
      },
      "source": [
        "You could use a larger epoch size here, but for the example in the Notebook, we'll stick with 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "un4ee2dM2WF4"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhClKkMLMZxW"
      },
      "source": [
        "Nothing left to do but #trainthatmodel!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8-oSdDB2Ymm"
      },
      "outputs": [],
      "source": [
        "warmup_steps = int(len(loader) * EPOCHS * 0.1)\n",
        "\n",
        "model.fit(\n",
        "    train_objectives=[(loader, loss)],\n",
        "    epochs=EPOCHS,\n",
        "    warmup_steps=warmup_steps,\n",
        "    output_path='exp_finetune',\n",
        "    show_progress_bar=True,\n",
        "    evaluator=evaluator,\n",
        "    evaluation_steps=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqfp0F8lMeal"
      },
      "source": [
        "### Evaluating our Embeddings Models\n",
        "\n",
        "Now that we've fine-tuned our embedding model on our data - lets see how it performs compared to the base embeddings, and OpenAI's `ada` embeddings!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTtogOHpAUnA"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "from llama_index.core import Settings, VectorStoreIndex\n",
        "from llama_index.core.schema import TextNode\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phzSwbIWAVPB"
      },
      "outputs": [],
      "source": [
        "TRAIN_DATASET_FPATH = './data/train_dataset.json'\n",
        "EVAL_DATASET_FPATH = './data/eval_dataset.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNygAxVNAYdT"
      },
      "outputs": [],
      "source": [
        "with open(TRAIN_DATASET_FPATH, 'r+') as f:\n",
        "    train_dataset = json.load(f)\n",
        "\n",
        "with open(EVAL_DATASET_FPATH, 'r+') as f:\n",
        "    eval_dataset = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvbEUNfPMrRu"
      },
      "source": [
        "We're going to leverage a \"hit rate\" for our evaluation.\n",
        "\n",
        "Basically what it says on the tin, \"hit rate\" is just a measure of how often we retrieve the correct relevant document.\n",
        "\n",
        "Since we have query/relevant document pairs, we can calculate this metric fairly easy.\n",
        "\n",
        "If the top-k retrieved results contain the correct context for our query - we hit!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skZBAZUXAbuT"
      },
      "outputs": [],
      "source": [
        "def evaluate(\n",
        "    dataset,\n",
        "    embed_model,\n",
        "    top_k=2,\n",
        "    verbose=False,\n",
        "):\n",
        "    corpus = dataset['corpus']\n",
        "    queries = dataset['queries']\n",
        "    relevant_docs = dataset['relevant_docs']\n",
        "\n",
        "    nodes = [TextNode(id_=id_, text=text) for id_, text in corpus.items()]\n",
        "    index = VectorStoreIndex(\n",
        "        nodes,\n",
        "        show_progress=True,\n",
        "        embed_model=embed_model\n",
        "    )\n",
        "    retriever = index.as_retriever(similarity_top_k=top_k)\n",
        "\n",
        "    eval_results = []\n",
        "    for query_id, query in tqdm(queries.items()):\n",
        "        retrieved_nodes = retriever.retrieve(query)\n",
        "        retrieved_ids = [node.node.node_id for node in retrieved_nodes]\n",
        "        expected_id = relevant_docs[query_id][0]\n",
        "        is_hit = expected_id in retrieved_ids  # assume 1 relevant doc\n",
        "\n",
        "        eval_result = {\n",
        "            'is_hit': is_hit,\n",
        "            'retrieved': retrieved_ids,\n",
        "            'expected': expected_id,\n",
        "            'query': query_id,\n",
        "        }\n",
        "        eval_results.append(eval_result)\n",
        "    return eval_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCmlLxW1ugI0"
      },
      "source": [
        "####üèóÔ∏è Activity #4:\n",
        "\n",
        "Describe what the `evaluate` function is doing in the above cell in natural language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZU3Rt8c5AcTH"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def evaluate_st(\n",
        "    dataset,\n",
        "    model_id,\n",
        "    name,\n",
        "):\n",
        "    corpus = dataset['corpus']\n",
        "    queries = dataset['queries']\n",
        "    relevant_docs = dataset['relevant_docs']\n",
        "\n",
        "    evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs, name=name)\n",
        "    model = SentenceTransformer(model_id)\n",
        "    return evaluator(model, output_path=\"/content/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GefF8XPoAwYu"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0nE_qX1Nrmg"
      },
      "source": [
        "### Ada Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLe8iTtXNkWt"
      },
      "source": [
        "We'll compare our results against OpenAI's `ada` model, so we'll need to load it up!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTUm_Ig9Adle"
      },
      "outputs": [],
      "source": [
        "ada = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
        "ada_val_results = evaluate(val_dataset, ada)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJPwRX6sAeu7"
      },
      "outputs": [],
      "source": [
        "df_ada = pd.DataFrame(ada_val_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8E0roD_AhA8"
      },
      "outputs": [],
      "source": [
        "hit_rate_ada = df_ada['is_hit'].mean()\n",
        "hit_rate_ada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKBjZ40dNtxA"
      },
      "source": [
        "### Base Embeddings Model Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7YK7fBHAjAu"
      },
      "outputs": [],
      "source": [
        "bge = \"local:BAAI/bge-small-en\"\n",
        "bge_val_results = evaluate(eval_dataset, bge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3AI0IvqAkHA"
      },
      "outputs": [],
      "source": [
        "df_bge = pd.DataFrame(bge_val_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIS3ejiLAlB3"
      },
      "outputs": [],
      "source": [
        "hit_rate_bge = df_bge['is_hit'].mean()\n",
        "hit_rate_bge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3_dVwuhAmJR"
      },
      "outputs": [],
      "source": [
        "evaluate_st(eval_dataset, \"BAAI/bge-small-en\", name='bge')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFQ6yFlmN9mc"
      },
      "source": [
        "### Fine-tuned Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dj98e1PKAnRX"
      },
      "outputs": [],
      "source": [
        "finetuned = \"local:exp_finetune\"\n",
        "eval_results_finetuned = evaluate(eval_dataset, finetuned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXNbn0lSAomd"
      },
      "outputs": [],
      "source": [
        "df_finetuned = pd.DataFrame(eval_results_finetuned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dMhPe7SAplo"
      },
      "outputs": [],
      "source": [
        "hit_rate_finetuned = df_finetuned['is_hit'].mean()\n",
        "hit_rate_finetuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ddpn0ErAquN"
      },
      "outputs": [],
      "source": [
        "evaluate_st(eval_dataset, \"exp_finetune\", name='finetuned')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1nWWOohN_e5"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Now we can compare the 3 embeddings models to see which performed the best!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpObNRolArwv"
      },
      "outputs": [],
      "source": [
        "df_ada['model'] = 'ada'\n",
        "df_bge['model'] = 'bge'\n",
        "df_finetuned['model'] = 'fine_tuned'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjEQtstoAsxj"
      },
      "outputs": [],
      "source": [
        "df_all = pd.concat([df_ada, df_bge, df_finetuned])\n",
        "df_all.groupby('model').mean('is_hit')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzMvHqbrOE4p"
      },
      "source": [
        "####üèóÔ∏è Activity #5:\n",
        "\n",
        "Determine the difference between the two types of embedding model's dimensions.\n",
        "\n",
        "- `text-embedding-ada-002` dimension: `ENTER DIMENSION HERE`\n",
        "- BGE Small dimension: `ENTER DIMENSION HERE`\n",
        "\n",
        "What does that communicate about our fine-tuning process, in your own words?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yse1ckCdAuLp"
      },
      "outputs": [],
      "source": [
        "df_st_bge = pd.read_csv('/content/Information-Retrieval_evaluation_bge_results.csv')\n",
        "df_st_finetuned = pd.read_csv('/content/Information-Retrieval_evaluation_finetuned_results.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOV9YTDnA4d2"
      },
      "outputs": [],
      "source": [
        "df_st_bge['model'] = 'bge'\n",
        "df_st_finetuned['model'] = 'fine_tuned'\n",
        "df_st_all = pd.concat([df_st_bge, df_st_finetuned])\n",
        "df_st_all = df_st_all.set_index('model')\n",
        "df_st_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwCPt0ObOMUU"
      },
      "source": [
        "Hopefully through this process you can see just how powerful this technique is!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
